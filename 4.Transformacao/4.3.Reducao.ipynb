{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At√© agora...\n",
    "---\n",
    "\n",
    "O objetivo deste curso √© **apresentar o Jupyter com reprodutibilidade para a realiza√ß√£o de projetos em Ci√™ncia de Dados**. Utilizando dessa ferramenta, estamos passando pelas principais etapas de um projeto de Ci√™ncia de Dados, e at√© agora vimos:\n",
    "\n",
    "- [Jupyter B√°sico](../2.Basico/2.1.Estrutura.Geral.ipynb)\n",
    "- [Prepara√ß√£o de Dados para Ci√™ncia](../3.Preparacao/3.1.Importacao.Dados.ipynb)\n",
    "- [Integra√ß√£o, Transforma√ß√£o, Redu√ß√£o](../4.Transformacao/4.1.Integracao.ipynb)\n",
    "\n",
    "Se voc√™ ainda est√° com d√∫vida em alguma dessas etapas e/ou algum conceito ainda n√£o est√° muito claro, por favor n√£o deixe de revisitar esses notebooks e, se necess√°rio, consulte o [texto do cap√≠tulo](https://sol.sbc.org.br/livros/index.php/sbc/catalog/view/67/292/544-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o pacote necess√°rio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Redu√ß√£o de Dados\n",
    "---\n",
    "\n",
    "Para finalizar a se√ß√£o de *Prepara√ß√£o de dados*, n√≥s vamos explorar a <ins>**Redu√ß√£o de dados**</ins>, uma etapa fundamental para lidar com grandes volumes de dados. De fato, gerenciar e processar dados requer tempo, esfor√ßo e recursos quando estamos lidando com alta dimensionalidade. \n",
    "\n",
    "Para enfrentar tais desafios, t√©cnicas de <ins>**Redu√ß√£o de dados**</ins> s√£o aplicadas. No entanto, embora sejam **essenciais**, essas t√©cnicas geralmente s√£o **complexas**, pois exigem amplo conhecimento para a escolha adequada de qual t√©cnica utilizar. \n",
    "\n",
    "Assim, para simplificar, essa etapa ir√° apresentar apenas uma t√©cnica muito utilizada de *Redu√ß√£o de dimensionalidade*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lise de Componentes Principais (PCA)\n",
    "\n",
    "O PCA, ou An√°lise de Componentes Principais, √© um dos m√©todos mais simples e, de longe, o mais comum para a *Redu√ß√£o da dimensionalidade*. \n",
    "\n",
    "Existem diversos motivos para aplicar a redu√ß√£o a partir do PCA. Mas, uma raz√£o bem comum √© quando coletamos muitos dados e n√£o sabemos por onde come√ßar nossas an√°lises. Assim, ao aplicar o PCA podemos reduzi-los em poucas dimens√µes e, em seguida, plot√°-los e estudar poss√≠veis padr√µes. \n",
    "\n",
    "> **Como ele funciona?** ü§î\n",
    "\n",
    "Considere a tabela `Tracks` que cont√©m informa√ß√µes sobre os *hits* do Spotify dos √∫ltimos anos. Nesta tabela, n√≥s temos extamente 14 caracter√≠sticas musicais. Ou seja, temos um conjunto de dados de **14 dimens√µes**. Certo?\n",
    "\n",
    "Imagine agora que voc√™ est√° preocupado que n√£o haja diferen√ßa em qualquer uma dessas caracter√≠sticas que nos ajude a classificar m√∫sicas entre *Solo* e *Colabora√ß√µes*. Portanto, precisamos investigar mais a fundo todas essas caracter√≠sticas.\n",
    "\n",
    "> **Mas como voc√™ plota dados de 14 dimens√µes?** ü§î\n",
    "\n",
    "√â exatamente a√≠ que o PCA entra para salvar o dia! üôå\n",
    "\n",
    "Em resumo, o m√©todo ir√° \"espremer\" essas 14 dimens√µes em, por exemplo, apenas **2** dimens√µes!!\n",
    "\n",
    "Ou seja, enquanto antes n√≥s t√≠nhamos cada m√∫sica associada a 14 caracter√≠sticas diferentes; agora, elas ter√£o apenas 2 vari√°veis associadas a elas. \n",
    "\n",
    "Assim, podemos plotar esses dois n√∫meros em um gr√°fico de dispers√£o, permitindo analisar se h√° alguma diferen√ßa entre as m√∫sicas *Solo* e *Colabora√ß√µes*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXEMPLO\n",
    "\n",
    "Especificamente, o PCA √© um algoritmo n√£o supervisionado que busca encontrar um meio de condensar a informa√ß√£o dos atributos originais do conjunto de dados em um conjunto menor de vari√°veis estat√≠sticas (**componentes principais**) com uma perda m√≠nima de informa√ß√£o. \n",
    "\n",
    "O n√∫mero de componentes principais se torna o n√∫mero de vari√°veis consideradas na an√°lise, mas geralmente as primeiras componentes s√£o as mais importantes j√° que explicam a maior parte da varia√ß√£o total dos dados.\n",
    "\n",
    "Para entender melhor como o PCA funciona, vamos utilizar o seguinte exemplo:\n",
    "\n",
    "#### 1. Criar o *DataFrame*\n",
    "\n",
    "Para exemplificar, n√≥s vamos utilizar as caracter√≠sticas ac√∫sticas da tabela `Tracks` e verificar se tais caracter√≠sticas podem ser √∫teis para classificar as m√∫sicas de acordo com o tipo delas (i.e., se s√£o m√∫sicas *Solo* ou *Colabora√ß√µes*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'duration_ms', 'key', 'mode', 'time_signature', 'acousticness',\n",
    "    'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness',\n",
    "    'speechiness', 'valence', 'tempo', 'song_type'\n",
    "]\n",
    "\n",
    "# Selecionando algumas colunas da tabela Tracks\n",
    "data = pd.read_table('../dataset/spotify_hits_dataset_complete.tsv', encoding='utf-8', usecols=cols)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape) # n√∫mero total de observa√ß√µes e atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pr√©-processamento dos dados\n",
    "\n",
    "Com o conjunto de dados criado, realizamos um pr√©-processamento dos dados:\n",
    "\n",
    "1. Separamos os dados em preditores (X) e vari√°vel *target* (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separando preditores (X) da vari√°vel target (y)\n",
    "y = data.song_type # vari√°vel target\n",
    "X = data.drop('song_type', axis=1) # conjunto de preditores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Padronizamos os preditores usando o transformador `StandardScaler` do m√≥dulo `sklearn.preprocessing` da biblioteca `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# Padronizando os dados de treino utilizando\n",
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **OBSERVA√á√ÉO!** Note que n√≥s padronizamos o conjunto de dados para que os atributos em maior escala n√£o dominem os novos componentes principais. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Aplicamos o algoritmo PCA\n",
    "\n",
    "Agora, podemos aplicar o algoritmo PCA. \n",
    "\n",
    "Para isso, iremos utilizar a biblioteca *sklearn* para importar o m√≥dulo `sklearn.decomposition` e a classe `PCA` para extrair os dois componentes principais `(n_components = 2)` do nosso conjunto de dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando os dois componentes principais\n",
    "pca_resultado = PCA(n_components=2)\n",
    "componentes = pca_resultado.fit_transform(X) # extraindo os dois componentes\n",
    "\n",
    "df_pcs = pd.DataFrame(componentes, columns=['PC1', 'PC2',])\n",
    "df_pcs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O *DataFrame* resultante apresenta os valores dos dois componentes principais para todas as 1284 amostras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Visualizar as duas dimens√µes\n",
    "\n",
    "Com a redu√ß√£o de dados realizada, agora podemos usar um gr√°fico de dispers√£o para analisar se h√° alguma diferen√ßa entre as m√∫sicas *Solo* e *Colabora√ß√µes*. Aqui, usaremos as bibliotecas `seaborn` e `matplotlib` que ser√£o abordadas com mais detalhes na se√ß√£o de **Visualiza√ß√£o de dados**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pcs['song_type'] = data['song_type'] # adicionando a coluna do tipo da m√∫sica\n",
    "\n",
    "# Visualizando os dois componentes principais resultantes \n",
    "sns.lmplot(x=\"PC1\", y=\"PC2\", data=df_pcs, fit_reg=False, hue='song_type', legend=False)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atrav√©s dessa an√°lise visual, podemos ver que, aparentemente, n√£o h√° diferen√ßa significativa entre as m√∫sicas *Solo* e *Colabora√ß√µes* em rela√ß√£o √†s suas caracter√≠sticas ac√∫sticas. Isso, porque os dois grupos demonstram comportamentos bem similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Vari√¢ncia explicada\n",
    "\n",
    "Ap√≥s a extra√ß√£o dos componentes principais, podemos verificar a quantidade de informa√ß√µes ou a varia√ß√£o que cada componente principal mant√©m ap√≥s projetar os dados em um subespa√ßo de dimens√£o inferior. Para isso, utilizamos a propriedade `explained_variance_ratio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Varia√ß√£o explicada por componentes principais: {}'.format(\n",
    "    pca_resultado.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como resultado, podemos observar que o primeiro componente principal det√©m 20,5% das informa√ß√µes, enquanto o segundo apenas 10,8% das informa√ß√µes. Juntos, os dois componentes cont√™m 30,85% das informa√ß√µes.\n",
    "\n",
    "Ou seja, ao reduzir a dimensionalidade do conjunto de dados para duas dimens√µes, 69,15% das informa√ß√µes originais foram perdidas. Uma solu√ß√£o para aumentar as informa√ß√µes seria aumentar o n√∫mero de dimens√µes (i.e., componentes principais) ao aplicar o PCA.\n",
    "\n",
    "Vamos testar com 10 dimens√µes üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando os dois componentes principais\n",
    "pca_resultado = PCA(n_components=10)\n",
    "componentes = pca_resultado.fit_transform(X) # extraindo os dois componentes\n",
    "\n",
    "print(f'Varia√ß√£o explicada por componentes principais: {round(sum(pca_resultado.explained_variance_ratio_)*100,1)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, aumentando o n√∫mero de dimens√µes, percebemos que as informa√ß√µes aumentam consideravelmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclus√£o\n",
    "\n",
    "Este notebook apresentou como reduzir a dimensionalidade dos dados utilizando o algoritmo PCA.\n",
    "\n",
    "üîé **Se interessou?** Para mais t√©cnicas de redu√ß√£o de dados, voc√™ pode dar uma olhada na biblioteca `sklearn`: [Unsupervised dimensionality reduction](https://scikit-learn.org/stable/modules/unsupervised_reduction.html)\n",
    "\n",
    "---\n",
    "\n",
    "Este foi o fim desta parte do tutorial sobre prepa√ß√£o de dados. A pr√≥xima parte ([5.Ciencia.de.Dados](../5.Ciencia.de.Dados/5.1.Ciencia.Dados.Basica.ipynb)) apresentar√° como fazer analise explorat√≥ria dos dados como parte da ci√™ncia de dados."
   ]
  }
 ],
 "metadata": {
  "julynter-results": {
   "filteredId": [],
   "filteredIndividual": [],
   "filteredRestart": [],
   "filteredType": [],
   "hash": "42e8b7591aa2173a7b72b5ae2fb6718ece06becc",
   "visible": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
